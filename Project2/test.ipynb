{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def getData():\n",
    "#    cwd = os.getcwd()  #getting the path of this current program\n",
    "#    filename = cwd + '/default of credit card clients.xls'  #path + file\n",
    "    \n",
    "np.random.seed(0)\n",
    "#Read file into pandas dataframe \n",
    "nanDict= {}\n",
    "df = pd.read_excel('default of credit card clients.xls', header=1, skiprows=0, index_col=0, na_values=nanDict)\n",
    "df.rename(index=str, columns={'default payment next month': 'defaultPaymentNextMonth'}, inplace=True)\n",
    "\n",
    "#Drop the rows including data where parameters are out of range\n",
    "df=df.drop(df[df.SEX<1].index)\n",
    "df=df.drop(df[df.SEX<2].index)\n",
    "df=df.drop(df[(df.EDUCATION <1)].index)\n",
    "df=df.drop(df[(df.EDUCATION >4)].index)\n",
    "df=df.drop(df[df.MARRIAGE<1].index)\n",
    "df=df.drop(df[df.MARRIAGE>3].index)\n",
    "\n",
    "#Features and targets\n",
    "#.values returns a numpy representation of the DataFrame\n",
    "X= df.loc[:, df.columns != 'defaultPaymentNextMonth'].values \n",
    "y= df.loc[:, df.columns == 'defaultPaymentNextMonth'].values\n",
    "    \n",
    "# Categorical variables to one-hot's\n",
    "onehotencoder = OneHotEncoder(categories=\"auto\")\n",
    "\n",
    "#OneHot encoder for column 1,2,3(,5,6,7,8,9,10)?? [sex,education,marriage, pay_april, pay_may, pay_jun, pay_jul, pay_aug, pay_sep]\n",
    "#Designmatrix, hotencoder on the categorical columns\n",
    "X = ColumnTransformer([('onehotencoder', onehotencoder, [1,2,3]),],remainder=\"passthrough\").fit_transform(X)\n",
    "y = np.ravel(y)\n",
    "#return X, np.ravel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "trainingShare = 0.8 \n",
    "seed = 1\n",
    "XTrain, XTest, yTrain, yTest=train_test_split(X, y, train_size=trainingShare,\n",
    "                                              test_size = 1-trainingShare,\n",
    "                                              random_state=seed)\n",
    "\n",
    "# Input Scaling\n",
    "sc     = StandardScaler()\n",
    "XTrain = sc.fit_transform(XTrain)\n",
    "XTest  = sc.transform(XTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have to one-hot the target vector??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot's of the target vector\n",
    "#Y_train_onehot, Y_test_onehot = onehotencoder.fit_transform(yTrain), onehotencoder.fit_transform(yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idunnmoatue/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/idunnmoatue/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.80\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89      4284\n",
      "           1       0.00      0.00      0.00      1073\n",
      "\n",
      "    accuracy                           0.80      5357\n",
      "   macro avg       0.40      0.50      0.44      5357\n",
      "weighted avg       0.64      0.80      0.71      5357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Testing full code for Neural Network, based on lecture notes 'Data Analysis and Machine Learning: Neural networks, from the simple perceptron to deep learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, X, y, n_hidden_neurons=50 , n_categories=10 , epochs=100 , batch_size=100 , eta=0.1 , lmbd=0.0):\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        #self.X_full = self.X\n",
    "        #self.y_full = self.y\n",
    "        \n",
    "        self.n_inputs         = X.shape[0]  #X-rows\n",
    "        self.n_features       = X.shape[1]  #X-columns \n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.n_categories     = n_categories\n",
    "        \n",
    "        self.epochs           = epochs\n",
    "        self.batch_size       = batch_size\n",
    "        self.iterations       = self.n_inputs // self.batch_size #Floor division\n",
    "        self.eta              = eta\n",
    "        self.lmbd             = lmbd\n",
    "        \n",
    "        self.create_biases_and_weights() #gives the initial bias and weigths. \n",
    "        self.feed_forward()\n",
    "        #print(self.hidden_weights)\n",
    "        #print(\"***\")\n",
    "        self.backpropagation()\n",
    "        #print(self.hidden_weights)        \n",
    "            \n",
    "    def sigmoid(self,x):\n",
    "        return(1/(1 + np.exp(-x)))\n",
    "        \n",
    "    def create_biases_and_weights(self):\n",
    "        self.hidden_weights = np.random.randn(self.n_features, self.n_hidden_neurons)\n",
    "        self.hidden_bias    = np.zeros(self.n_hidden_neurons) + 0.01\n",
    "        self.output_weights = np.random.randn(self.n_hidden_neurons, self.n_categories)\n",
    "        self.output_bias    = np.zeros(self.n_categories) + 0.01\n",
    "    \n",
    "    def feed_forward(self):\n",
    "        #feed_forward training\n",
    "        self.z_h = np.matmul(self.X, self.hidden_weights) + self.hidden_bias\n",
    "        self.a_h = self.sigmoid(self.z_h)\n",
    "        \n",
    "        self.z_o = np.matmul(self.a_h, self.output_weights) + self.output_bias\n",
    "        \n",
    "        exp_term = np.exp(self.z_o)\n",
    "        self.probabilities = exp_term /np.sum(exp_term, axis=1, keepdims=True)\n",
    "        \n",
    "    \n",
    "    def feed_forward_output(self, X):\n",
    "        #feed_forward for output\n",
    "        z_h = np.matmul(X, self.hidden_weights) + self.hidden_bias\n",
    "        a_h = self.sigmoid(z_h)\n",
    "        \n",
    "        z_o = np.matmul(a_h, self.output_weights) + self.output_bias\n",
    "\n",
    "        exp_term = np.exp(z_o)\n",
    "        probabilities = exp_term/ np.sum(exp_term, axis=1, keepdims=True)\n",
    "        return probabilities\n",
    "    \n",
    "    def backpropagation(self):\n",
    "   \n",
    "        error_output = self.probabilities - self.y  #Error in output layer delta^L\n",
    "        error_hidden = np.matmul(error_output, self.output_weights.T) *self.a_h * (1-self.a_h) #Error for the jth neuron in lth layer delta^l\n",
    "        \n",
    "        self.output_weights_gradient = np.matmul(self.a_h.T, error_output)\n",
    "        self.output_bias_gradient    = np.sum(error_output, axis=0)\n",
    "        \n",
    "        self.hidden_weights_gradient = np.matmul(self.X.T, error_hidden)\n",
    "        self.hidden_bias_gradient    = np.sum(error_hidden, axis=0)\n",
    "        \n",
    "        \n",
    "        if self.lmbd > 0.0:\n",
    "            self.output_weights_gradient += self.lmbd * self.output_weights\n",
    "            self.hidden_weights_gradient += self.lmbd * self.hidden_weights\n",
    "            \n",
    "        self.output_weights -= self.eta * self.output_weights_gradient\n",
    "        self.output_bias    -= self.eta * self.output_bias_gradient\n",
    "        self.hidden_weights -= self.eta * self.hidden_weights_gradient\n",
    "        self.hidden_bias    -= self.eta * self.hidden_bias_gradient \n",
    "        \n",
    "       \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        probabilities = self.feed_forward_output(X)\n",
    "        return np.argmax(probabilities, axis = 1)\n",
    "    \n",
    "    def predict_probabilities(self, X):\n",
    "        probabilities = self.feed_forward_output(X)\n",
    "        return probabilities\n",
    "    \n",
    "    \n",
    "    \n",
    "    #def train(self):\n",
    "     #   data_ind = np.arange(self.n_inputs)\n",
    "      #  print(\"hellu\")\n",
    "       # for i in range(self.epochs):\n",
    "        #    for j in range(self.iterations):\n",
    "                #pick datapoints with replacements\n",
    "         #       chosen_datapoints = np.random.choice(data_ind, size=self.batch_size, replace=False)\n",
    "                \n",
    "                #minibatch training data\n",
    "          #      self.X_full = self.X[chosen_datapoints]\n",
    "           #     self.y_full = self.y[chosen_datapoints]\n",
    "                \n",
    "            #    self.feed_forward()\n",
    "             #   self.backpropagation()\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "          \n",
    "  \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idunnmoatue/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:55: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN = NeuralNetwork( X, y, n_hidden_neurons=50, n_categories=10 , epochs=100 , batch_size=100 , eta=0.1 , lmbd=0.0)\n",
    "\n",
    "NN.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
